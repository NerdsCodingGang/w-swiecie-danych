---
title: 14. PorÃ³wnywanie modeli i ocena jakoÅ›ci
layout: post
---

Do tej pory trenowaliÅ›my pojedyncze modele i sprawdzaliÅ›my, jak przewidujÄ… popularnoÅ›Ä‡ utworÃ³w.  
Teraz zrobimy coÅ›, co w pracy analitycznej jest absolutnie kluczowe:

**sprawdzimy, ktÃ³ry model radzi sobie z zadaniem najlepiej oraz dlaczego.**

To bardzo waÅ¼ny moment, bo w Å›wiecie ML nie istnieje coÅ› takiego jak najlepszy model do wszystkiego, uniwersalnych rozwiÄ…zaÅ„ nie ma... ;)  
Czasem dziaÅ‚a liniowy, czasem drzewo decyzyjne, czasem model zespoÅ‚owy (taki jak RandomForest).  
Dlatego musimy nauczyÄ‡ siÄ™ **porÃ³wnywaÄ‡ ich skutecznoÅ›Ä‡**.

---

## Train/Test split ğŸ¤” po co dzielimy dane?

Kiedy model uczymy na *wszystkich danych*, to moÅ¼e wydawaÄ‡ siÄ™ Å›wietny,  
ale w rzeczywistoÅ›ci mÃ³gÅ‚ po prostu â€nauczyÄ‡ siÄ™ na pamiÄ™Ä‡â€.

Å»eby temu zapobiec, dzielimy dane na dwie czÄ™Å›ci:

- **train** â€“ do nauki modelu  
- **test** â€“ do sprawdzenia, jak model radzi sobie na danych, ktÃ³rych *nigdy jeszcze nie widziaÅ‚*

ZaÅ‚Ã³Å¼my sytuacjÄ™, Å¼e posiadasz ogromnÄ… bazÄ™ obrazkÃ³w. W tej bazie mamy obrazki pieskÃ³w i kotkÃ³w, baza ma tysiÄ…ce zdjÄ™Ä‡, model zaczyna uczyÄ‡ siÄ™Â perfekcyjnie na danych treningowych. Potem okazuje siÄ™, Å¼e model wcale tak Å›wietnie nie radzi sobie na danych testowych. 

Dlaczego? 

MoÅ¼e w danych treningowych pojawiÅ‚y siÄ™Â podpisy, albo metadane np. wszystkie pliki pies miaÅ‚y nazwÄ™ `IMG_DOG_5134324.png` w takim wypadku model wystarczyÅ‚o, Å¼e nauczyÅ‚ siÄ™, Å¼e nazwa zawiera pewne sÅ‚owo... takich sytuacji moÅ¼e byÄ‡Â wiÄ™cej, w naszym obowiÄ…zku jest zadbanie o to, Å¼e nasz model zostaÅ‚ przetestowany przeciwko prawdziwej rzeczywistoÅ›ci.


A tylko **test** pokazuje rzeczywistÄ… jakoÅ›Ä‡ modelu.

W Pythonie wyglÄ…da to tak:

```python
from sklearn.model_selection import train_test_split  

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
```

To oznacza:

- 80 procent danych â†’ trenowanie  
- 20 procent danych â†’ testowanie

---

## Jakie modele porÃ³wnamy?

Zrobimy trzy modele przewidujÄ…ce popularnoÅ›Ä‡ utworÃ³w:

1. **LinearRegression** â€“ model liniowy  
2. **DecisionTreeRegressor** â€“ drzewo decyzyjne  
3. **RandomForestRegressor** â€“ wiele drzew poÅ‚Ä…czonych w jeden model  

KaÅ¼dy model dziaÅ‚a inaczej, ma inne mocne strony i inne sÅ‚aboÅ›ci.

---

## Jak oceniaÄ‡ jakoÅ›Ä‡ modeli?

UÅ¼yjemy trzech wskaÅºnikÃ³w jakoÅ›ci:

### ğŸ‘‰  MAE â€“ Mean Absolute Error  
Åšrednia rÃ³Å¼nica miÄ™dzy przewidywanÄ… a prawdziwÄ… wartoÅ›ciÄ….  
Im mniejsza, tym lepiej.  
Åatwe do zrozumienia: â€o ile model siÄ™ myli *Å›rednio*â€.

### ğŸ‘‰ MSE â€“ Mean Squared Error  
To samo co MAE, ale bardziej karze wiÄ™ksze bÅ‚Ä™dy.  
RÃ³wnieÅ¼ â€“ im mniejsza, tym lepiej.

### ğŸ‘‰ RÂ² â€“ wspÃ³Å‚czynnik dopasowania  
JuÅ¼ kolejny nam siÄ™Â pojawia. WartoÅ›Ä‡ od 0 do 1.  
Im bliÅ¼ej 1, tym lepiej model wyjaÅ›nia zmiennoÅ›Ä‡ danych.

RÂ² jest najczÄ™Å›ciej uÅ¼ywany, ale najlepiej patrzeÄ‡ na wszystkie trzy miary.



## Trzy modele, jeden podziaÅ‚ danych, trzy wyniki

```python
from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# Dane
X = df_small[["tempo", "energy", "danceability"]]
y = df_small["popularity"]

# Train/Test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

models = {
    "LinearRegression": LinearRegression(),
    "DecisionTree": DecisionTreeRegressor(),
    "RandomForest": RandomForestRegressor()
}

for name, model in models.items():
    model.fit(X_train, y_train)
    predictions = model.predict(X_test)

    mae = mean_absolute_error(y_test, predictions)
    mse = mean_squared_error(y_test, predictions)
    r2 = r2_score(y_test, predictions)

    print(f"\n=== {name} ===")
    print("MAE:", mae)
    print("MSE:", mse)
    print("RÂ²:", r2)
```

###  Jak interpretowaÄ‡ wyniki?

Po uruchomieniu kodu zobaczysz 3 zestawy liczb.  

PrzykÅ‚adowo Twoje wnioski 

- `LinearRegression` â€“ dziaÅ‚a stabilnie, ale moÅ¼e nie uchwyciÄ‡ skomplikowanych zaleÅ¼noÅ›ci  
- `DecisionTree` â€“ moÅ¼e radziÄ‡ sobie Å›wietnie na train, ale gorzej na test (to typowy overfitting)  
- `RandomForest` â€“ zwykle najbardziej praktyczny, bo uÅ›rednia wynik wielu drzew, nie zawsze tego chcemy

NajwaÅ¼niejsze:
**porÃ³wnujemy modele po tych samych danych i tej samej miarze jakoÅ›ci.**

wiemy, Å¼e:

- jeden model nie dominuje zawsze  
- kaÅ¼dy ma swoje ograniczenia  
- warto je porÃ³wnywaÄ‡ i myÅ›leÄ‡ o kompromisach, kosztach i zyskach
- dopiero sprawdzenie na jakoÅ›ci testowej pokazuje prawdziwÄ… skutecznoÅ›Ä‡


## ZADANIE - ktÃ³ry model dziaÅ‚a najlepiej?

Teraz czas przeÄ‡wiczyÄ‡ to samodzielnie.  
Skoro porÃ³wnaliÅ›my trzy modele na naszych danych Spotify, zrÃ³bmy maÅ‚y krok dalej.

Twoim zadaniem bÄ™dzie:

ğŸŸ£**1. WybraÄ‡ **jednÄ… dodatkowÄ… cechÄ™** do modelu**
MoÅ¼esz wybraÄ‡ np.:

- `loudness`  
- `valence`  
- `duration_ms`  
- `acousticness`

Dodaj jÄ… do tabeli `X`.  
ZastanÃ³w siÄ™:  czy ta cecha *moÅ¼e* mieÄ‡ wpÅ‚yw na popularnoÅ›Ä‡?


ğŸŸ£ **2. Ponownie uruchomiÄ‡ trzy modele**
LinearRegression, DecisionTreeRegressor, RandomForestRegressor  
â€” tak jak w przykÅ‚adzie powyÅ¼ej, ale z nowÄ… cechÄ… w `X`.

Nic nie zmieniasz oprÃ³cz listy cech.


ğŸŸ£ **3. PorÃ³wnaÄ‡ wyniki (MAE, MSE, RÂ²)**
Zapisz w komÃ³rce tekstowej:

- ktÃ³ry model dziaÅ‚a najlepiej,  
- ktÃ³ry najsÅ‚abiej,  
- czy dodanie cechy pomogÅ‚o, czy nie?


ğŸŸ£ **4. Dodatkowo (opcjonalnie)**

Narysuj wykres sÅ‚upkowy z wartoÅ›ciami RÂ² dla trzech modeli.

PodpowiedÅº:

> plt.bar(nazwy_modeli, wyniki_r2)


---
## Gratulacje ğŸ‰ 

JeÅ›li coÅ› z tych 2-dni naprawdÄ™ warto zapamiÄ™taÄ‡, to to, Å¼e w pracy z danymi nie chodzi o perfekcje (nie zawsze) i czÄ™sto wcale nie perfekcyjnoÅ›Ä‡, a doÅ›Ä‡ dobrze, tak by zrozumieÄ‡ ukrytÄ… informacjÄ™! W duÅ¼ych danych trudno o perfekcje, tylko o umiejÄ™tnoÅ›Ä‡ zadawania wÅ‚aÅ›ciwych pytaÅ„ i sprawdzania ich na faktach.

WÅ‚aÅ›nie zrobiliÅ›cie to, co robiÄ… osoby pracujÄ…ce w data science na co dzieÅ„: wziÄ™liÅ›cie dane, zadaliÅ›cie im pytania, zbudowaliÅ›cie modele i oceniliÅ›cie ich jakoÅ›Ä‡!


![applause]({{ site.baseurl }}/assets/applause.gif){:title="brawo" class="img-responsive"}
